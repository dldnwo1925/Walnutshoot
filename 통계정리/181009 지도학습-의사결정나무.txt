# 의사결정나무 알고리즘
- 지니 불순도(Gini Impurity) 또는 정보 이득(Information Gain)등의 기준을 사용해 노드를 재귀적으로 분할
  하면서 나무 모델을 만드는 방법. 의사 결정 나무는 if-else 조건문과 같은 형식이어서 이해하기 쉽고 속도가 빠르며
  여러 Feature간 상호 작용을 잘 표현해주고 다양한 데이터 유형에 사용할 수 있다는 장점이 있음.
- 특히 나무 모델중 랜덤 포레스트는 괜찮은 성능을 보여주어 머신 러닝 대회에서도 기본이 되는 알고리즘으로 자주 제시됨.
  (요즘 성능 최고는 XGboost가 제일 성적 잘 나오는듯 2018.10)

# 의사결정나무 모델?
- 데이터의 특징에 대한 질문을 하며 응답에 따라 데이터를 분류해가는 알고리즘.
- 알고리즘 모형처럼 Yes or No 질문을 반복하며 진행. 반복된 분류를 통해 집단을 찾음.
- 각 단계 질문이 상위 단계의 질문과 연관성이 있기 때문에 의사결정나무 모델은 Feature의 연관성을 잘 표현함.

※ 노드를 나누는 기준.
- 노드 중 분류의 시작점이 해당하는 최상된에 위치한 노드를 Root Node, 더 이상 자식 노드가 없는 최하위 노드를
  Leaf Node라고 함.
- 의사결정나무 모델은 각 노드마다 질문을 하고 응답에 따라 가지를 분류해 데이터를 분리한다.
- 데이터가 얼마나 잘 분리되었는지는 불순도(impurity)라는 기준으로 평가하며 가장 좋은 질문은 한 노드의 데이터를
  두 개의 자식 노드로 분리했을 때 자식 노드들의 불순도가 가장 낮아지는 질문이다.
- 불순도는 노드에 여러 분류가 섞여 있을수록 높다. 반면 하나의 분류만 있다면 낮다. 
- 가장 흔히 사용하는 불순도 함수는 지니 불순도이다.

※ 지니 불순도
- 계산 식 : I(A) = (i=1부터 C까지 시그마)f(PiA)
- C는 분류의 개수, PiA는 노드 A에 속한 분류 i인 표본의 비율.
- 함수 식 : f(p) = p(1-p)
- 지니 불순도는 p=0 또는 o=1일 때 0이고 p=1/2일 때 가장 큰 값을 가지는 포물선이다.
- 1/2 일 때 가장 분류가 잘 안된 것.
- 0~0.5값을 가질 때 0에 가까울수록 분류가 잘 되었다고 볼 수 있음.

# R프로그래밍으로 실습 확인.



