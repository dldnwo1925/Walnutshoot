# 앙상블 알고리즘
1. 목적 : 다양한 모형의 예측 결과를 결합함으로 단일 모형으로 분석했을 떄 보다 신뢰성 높은 예측값을 얻는다.
- 예로 두 집단을 분류하는 5개의 분류기가 각각 오분류율이 5%라고 가정할 때 해당 모형들이 모두 동일한 결정을 내린다고
  하면 모형의 오분류율은 5%가 된다.
- 하지만 각각의 분류기가 상호독립적이어 전체 분류기으 ㅣ절반 이상이 오분류를 하는 경우에 앙상블 모형의 오분류율은
  0.01%로 떨어지게 됨.

2. 앙상블의 조건
- 각각의 분류기는 상호 독립적이어야 한다.
- 각 분류기의 오분류율은 적어도 50%보다는 낮아야 함.

3. 앙상블 모형의 종류

3-1. 데이터를 조정하는 방법
- 적절한 표본추출 방법을 활용하여 어러 개의 훈련용 데엍 집합을 생성함.
- 각각의 데이터 집합을 활용하여 어러 개의 분류기를 생성해 앙상블을 진행
(Bagging, Boosting)

3-2. 변수의 개수를 조절하는 방법.(Random Forest)
- 전체 변수집합에서 부분 변수집합을 선택하여 훈련용 데이터 생성
- 각각의 데이터 집합에 대해 분류기를 생성한 후 앙상블을 진행.

# 앙상블 모형
- 여러 개의 분류모형에 의한 결과를 종합하여 분류의 정확도를 높이는 알고리즘
- 이는 적절한 표본추출법으로 데이터에서 여러 개의 훈련용 데이터 집합을 만들어 각각의 데이터 집합에서 하나의
  분류기를 만들어 앙상블 하는 방법이다. 즉 새로운 자료에 댛나 분류 예측값들의 가중 투표를 통해 분류를 수행함.
- 데이터를 조절하는 가장 대표적인 방법에는 배깅과 부스팅이 있다. 랜덤 포레스트는 배깅의 개념과 속성의 임의 선택
  을 결합한 앙상블 기법이다.

  ※ 장점 
	1. 평균을 취함으로써 편의를 제거해 줌 - 치우침이 있는 여러 모형의 평균을 취하면 어느 쪽에도 치우치지 않는
	결과(평균)을 얻게 된다.
	2. 분산을 감소시킨다 - 한 개 모형으로부터의 단일 의견보다 여러 모형의 의견을 결합하면 변동이 작아짐.
	3. 과적합의 가능성을 줄여준다 - 과적합이 없는 각 모형으로부터 예측을 결합(평균, 가중평균, 로지스틱회귀)하면
	과적합의 여지가 줄어듦.


@ 여러개의 모델의 결과를 활용하는 것이 아닌 훈련 집합을 조작함으로써 성능 향상을 기대한다.
# Bagging(Bootstrap aggregation의 약어)
- 훈련집합들 중 훈련집합 뽑는 방법을 균일한 확률 분포에 따라 반복적으로 샘플링해 모델을 여러 개 만듬.
- 생성된 여러 모델을 바탕으로 앞서 수행한 같은 방법으로 클래스를 선택함.
ex) 1000개의 Observation을 가진 데이터 셋이 있을 때 1000개를 한 번에 쓰는게 아닌 일정 비율의 데이터 셋을
    뽑아 학습 데이터 셋을 여러개 만들고 이를 모두 수행한 결과값의 평균, 과반 투표등으로 최적화된 모델을 만듦.
	여러 학습 데이터 모델을 이용하는 것.
	
# Boosting
- 모델을 생성할 때 이전 모델에서 잘못 분류한 학습 데이터를 다음 모델이 생성 될 때 학습집합으로 선택되어질 가능성을
  높여주어 다음 모델에서 오차에 대한 보완을 유도하는 방법.
- 학습 데이터 중 일부를 랜덤으로 추출하여 모델을 만듦 > 모델의 정확도를 측정한 후 에러가 난 데이터 탐색 >
  이 데이터들에게 가중치를 더 준 상태에서 다음 모델을 만들 데이터를 랜덤으로 추출(이 과정에서 에러 난 데이터 포함
  확률이 증가함) > 모델 생성 > 정확도 측정 반복
- 각 모델 결과값의 평균 또는 과반 투표로 조합하여 최종 모델을 만든다.
- 배깅의 과정과 유사하지만 가장 큰 차이점이 무작위 확률로 표본을 재구성하는 것이 아닌 오류발견된 데이터에 가중치 부여
  하는 것이 가장 큰 차이점이다.
- 부스팅 기법중 가장 많이 사용되는것이 아다부스팅(AdaBoosting : adaptive boosting)알고리즘임.


# 베이지안 알고리즘
- 베이지안 확률 모델은 전통적인 피셔리언 확률 모델인 빈도주의와 함께 현대 확률 통계학의 중요한 축을 구성함.
- 빈도주의가 오차범위, 통계적 유의성 검증 등으로 불확실성을 객관적으로 제거해 나갈 수 있다고 믿은 반면
  베이지안 확률 모델은 주관적인 추론을 바탕으로 한 사전확률을 추가적 관측으로 사후확률로 업데이트 해 나가는 방법으로
  불확실성을 제거해 나가려는 접근 방법을 취한다.
- 베이즈 추론을 기반으로 한 방법론의 정확도는 일반적으로 머신러닝 대표 기법인 랜덤 포레스트나 트리 분류 방법 보다
  높다고 평가 받는다.

# naiveBayes 분류란?
- 베이지안 기법 기반의 분류기는 훈련 데이터를 활용해 특정 값이 제공하는 증거 기반으로 결과가 관측될 확률을 계사나함.
# naiveBayes 언제 사용되는지?
- 결과에 대한 전체 확률을 추정하기 위해 동시에 여러 속성 정보를 고려해야만 하는 문제에 가장 적합하다.
- 모든 사건이 독립이라면 다른 사건을 관측해 어떤 사건을 예측하는 것을 불가능하다.
  다시 말해, 종속사건이 예측모델링 기반이 된다.
- 단순베이즈분류는 문서분류, 의료진단 등에 많이 사용됨.
- 사후 확률이 가장 큰 집단으로 새로운 데이터를 분류하게 된다.
- 조건부 독립의 가정이 비현실적인 측면이 있으나 계산이 간편하여 널리 이용되고 있다.
- 단순베이즈분류(naive Bayes Classification)모형은 베이즈 정리에 기반한 방법으로 사후 확률의 계산 시
  조건부 독립을 가정하여 계산을 단순화한 방법이다.
- 적절한 전처리를 거친 단순베이즈분류는 SVM(Support Vector Machine)을 포함한 보다 발전된 기법과도
  경쟁이 된다.

# 단순베이즈분류(naive Bayes Classification)
- 연속형 또는 이산형에 관계없이 임의 크기의 예측변수를 다룰 수 있다.
- 데이터가 x=(x1,x2,x3,x....)으로 주어질 때 이 데이터가 특정 집단에서 나왔을 사후확률을 공식을 통해 구함.

# 단순베이즈분류 결측값을 포함하는 자료일 경우.
- 훈련단계 : 속성값-군집 조합에 대한 빈도 계산 시 결측값을 포함하는 케이스가 제외된다.
- 분류단계 : 결측인 속성이 계산과정에서 생략된다.
- 단순베이즈분류에서는 결측값에 대한 처리가 매우 유연하게 이루어진다.
- 모형구축에서는 결측값을 포함하는 케이스 제외, 분류과정에서는 결측 속성에 대한 확률만 계산에서 제외된다고 알아두면 된다.







  

